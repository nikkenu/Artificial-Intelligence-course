{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### AI Agents\n",
    "* Simple Reflex Agent\n",
    "  * Take decisions on the basis of the current percepts and ignore the rest.\n",
    "  * Succees only in fully observable environment.\n",
    "  * Works on condition-action rule.\n",
    "  * Very limited intelligence.\n",
    "  * Most of the times too big to store.\n",
    "  * Not adaptive to changes in the environment.\n",
    "* Model-based reflex agent\n",
    "  * Can work in a partially observable environment and track the situation.\n",
    "  * Two important factors:\n",
    "    * Model: It is knowledge about how things happen in the world.\n",
    "    * Internal State: It is a representation of the current state based on percept history.\n",
    "* Goal-based agent\n",
    "  * Not always sufficient to decide for an agent what to do.\n",
    "  * Needs to know its goal which describes desirable situations.\n",
    "  * They choose an action, so that they can achieve the goal.\n",
    "* Utility-based agent\n",
    "  * Similar to goal-based agent but provide an extra component of utility measurement which makes them different by providing a measure of success at a given state.\n",
    "  * Act based on the best way to achieve the goal.\n",
    "  * Useful when there are multiple possible alternatives.\n",
    "* Learning agent\n",
    "  * It can learn from its past experiences, or it has learning capabilities.\n",
    "  * Starts with basic knowledge that learn through learning.\n",
    "  * Four conceptual components:\n",
    "    * Learning element: Responsible to making improvements\n",
    "    * Critic: Learning element takes feedback from critic, which desribes how well the agent is doing.\n",
    "    * Performance element: Responsible for selecting external action.\n",
    "    * Problem generator: Responsible for suggesting actions that will lead to new and informative experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    "\n",
    "* Brute-force search / exhaustive search\n",
    "  * General problem-solving technique.\n",
    "  * Systematically go through the possible states.\n",
    "  * The worst setting is that it examines all possible states of the search space.\n",
    "  * If state space is large, exhaustive search is very slow.\n",
    "\n",
    "* Blind search strategies\n",
    "  * Breadth First Search (BFS)\n",
    "    * Has to hold on to all nodes it has seen until the goal state is found.\n",
    "    * Guarantees finding a goal if exists.\n",
    "  * Depth First Search (DFS)\n",
    "    * May discard paths that have already been examined.\n",
    "    * May lose itself to an infinite search path.\n",
    "    * Might miss the shallowest goal state.\n",
    "  * Uniform-Cost Search\n",
    "    * Similar to Dijikstra's algorithm.\n",
    "    * Instead of inserting all vertices into a priority queue, we insert only source, and then one by one insert when needed.\n",
    "    * Optimal! \n",
    "    * No information about goal location.\n",
    "    * Explores options in every direction.\n",
    "\n",
    "* A*\n",
    "  * Is a graph traversal and path search algorithm.\n",
    "  * Used often because of its completeness, optimality and optimal efficiency.\n",
    "  * At each iteration of its main loop, algorithm needs to determine which of its paths to extend.\n",
    "  * Everything is based on the cost of the path and estimate of the cost required to extend the path all the way to the goal.\n",
    "  * f(n) = g(n) + h(n), where n is the next node on the path, g(n) is the cost of the path from the start node to n and h(n) is heuristic function that estimates the cost of the cheapest path from n to the goal.\n",
    "  * Typically uses priority queue. \n",
    "  * h(n) never overestimates the cost to reach the goal.\n",
    "\n",
    "* Beyond Classical Search\n",
    "  * Local Seach\n",
    "    * Tree search keeps unexplored alternatives on the fringe.\n",
    "    * Improve a single option until you can't make it better.\n",
    "    * Fast and memory efficient, but incomplete and suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3\n",
    "\n",
    "Value of a State: The best achievable outcome (utility) from that state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.0 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'C:/Users/nikla/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def max_value(state):\n",
    "    #Initialize v to minus INFINITY\n",
    "    v = float('-inf')\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1771f51d2793012f57bcacf2fc640cb41823a184b829a5c09f37ca2024b4c54b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
